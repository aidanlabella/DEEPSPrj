@incollection{kaufmann_evolving_2019,
	address = {Cham},
	title = {Evolving {Recurrent} {Neural} {Networks} for {Time} {Series} {Data} {Prediction} of {Coal} {Plant} {Parameters}},
	volume = {11454},
	isbn = {978-3-030-16691-5 978-3-030-16692-2},
	url = {http://link.springer.com/10.1007/978-3-030-16692-2_33},
	abstract = {This paper presents the Evolutionary eXploration of Augmenting LSTM Topologies (EXALT) algorithm and its use in evolving recurrent neural networks (RNNs) for time series data prediction. It introduces a new open data set from a coal-ﬁred power plant, consisting of 10 days of per minute sensor recordings from 12 diﬀerent burners at the plant. This large scale real world data set involves complex depedencies between sensor parameters and makes for challening data to predict. EXALT provides interesting new techniques for evolving neural networks, including epigenetic weight initialization, where child neural networks re-use parental weights as a starting point to backpropagation, as well as node-level mutation operations which can improve evolutionary progress. EXALT has been designed with parallel computation in mind to further improve performance. Preliminary results were gathered predicting the Main Flame Intensity data parameter, with EXALT strongly outperforming ﬁve traditional neural network architectures on the best, average and worst cases across 10 repeated training runs per test case; and was only slightly behind the best trained Elman recurrent neural networks while being signiﬁcantly more reliable (i.e., much better average and worst case results). Further, EXALT achived these results 2 to 10 times faster than the traditional methods, in part due to its scalability, showing strong potential to beat traditional architectures given additional runtime.},
	language = {en},
	urldate = {2024-04-02},
	booktitle = {Applications of {Evolutionary} {Computation}},
	publisher = {Springer International Publishing},
	author = {ElSaid, AbdElRahman and Benson, Steven and Patwardhan, Shuchita and Stadem, David and Desell, Travis},
	editor = {Kaufmann, Paul and Castillo, Pedro A.},
	year = {2019},
	doi = {10.1007/978-3-030-16692-2_33},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {488--503},
	file = {ElSaid et al. - 2019 - Evolving Recurrent Neural Networks for Time Series.pdf:/Users/aidan/Zotero/storage/FX2XPBX3/ElSaid et al. - 2019 - Evolving Recurrent Neural Networks for Time Series.pdf:application/pdf},
}


@inproceedings{ororbia_investigating_2019,
	address = {New York, NY, USA},
	series = {{GECCO} '19},
	title = {Investigating recurrent neural network memory structures using neuro-evolution},
	isbn = {978-1-4503-6111-8},
	url = {https://doi.org/10.1145/3321707.3321795},
	doi = {10.1145/3321707.3321795},
	abstract = {This paper presents a new algorithm, Evolutionary eXploration of Augmenting Memory Models (EXAMM), which is capable of evolving recurrent neural networks (RNNs) using a wide variety of memory structures, such as Δ-RNN, GRU, LSTM, MGU and UGRNN cells. EXAMM evolved RNNs to perform prediction of large-scale, real world time series data from the aviation and power industries. These data sets consist of very long time series (thousands of readings), each with a large number of potentially correlated and dependent parameters. Four different parameters were selected for prediction and EXAMM runs were performed using each memory cell type alone, each cell type and simple neurons, and with all possible memory cell types and simple neurons. Evolved RNN performance was measured using repeated k-fold cross validation, resulting in 2420 EXAMM runs which evolved 4, 840, 000 RNNs in {\textasciitilde}24,200 CPU hours on a high performance computing cluster. Generalization of the evolved RNNs was examined statistically, providing findings that can help refine the design of RNN memory cells as well as inform future neuro-evolution algorithms.},
	urldate = {2024-04-01},
	booktitle = {Proceedings of the {Genetic} and {Evolutionary} {Computation} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Ororbia, Alexander and ElSaid, AbdElRahman and Desell, Travis},
	month = jul,
	year = {2019},
	keywords = {neuroevolution, recurrent neural networks, time series data},
	pages = {446--455},
	file = {Submitted Version:/Users/aidan/Zotero/storage/9WCRETDD/Ororbia et al. - 2019 - Investigating recurrent neural network memory stru.pdf:application/pdf},
}

@misc{misc_air_quality_360,
  author       = {Vito,Saverio},
  title        = {{Air Quality}},
  year         = {2016},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: https://doi.org/10.24432/C59K5F}
}

@article{belavadi2020air,
    title={Air quality forecasting using LSTM RNN and wireless sensor networks},
    author={Belavadi, Sagar V and Rajagopal, Sreenidhi and Ranjani, R and Mohan, Rajasekar},
    journal={Procedia Computer Science},
    volume={170},
    pages={241--248},
    year={2020},
    publisher={Elsevier}
}

@article{lyu2023online,
  title={Online evolutionary neural architecture search for multivariate non-stationary time series forecasting},
  author={Lyu, Zimeng and Ororbia, Alexander and Desell, Travis},
  journal={Applied Soft Computing},
  volume={145},
  pages={110522},
  year={2023},
  publisher={Elsevier}
}

